{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Driving Accidents Database\n",
    "\n",
    "### <font color=green>*Udacity Data Engineering Nanodegree - Capstone Project*</font>\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to build a database that analysts at the Department for Transport, of the United Kingdom Government will be able to use.\n",
    "The database will cover information pertaining to road traffic accidents recorded, with accompanying information, such as location data.\n",
    "The database will be designed in a star-schema format, where accident records will provide the FACT table, and accompanying reference information will form the DIMENSION tables.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "-----------------\n",
    "\n",
    "#### Project Environment\n",
    "This project is managed through a conda environment. Where required, you may need to `conda` or `pip` install specific packages into your own evironment. <br>\n",
    "`pip install` commands that can be run from the notebook to the terminal will be place below, but commented out. You can un-comment these and install as required to your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install commands if needed\n",
    "#!pip install ipython-sql\n",
    "#!pip install pandas\n",
    "#!pip install json\n",
    "#!pip install os\n",
    "#!pip install glob\n",
    "#!pip install zipfile\n",
    "#!pip install boto3\n",
    "#!pip install botocore\n",
    "#!pip install configparser \n",
    "#!pip install psycopg2\n",
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "#import kaggle\n",
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import glob \n",
    "import zipfile \n",
    "import boto3\n",
    "from botocore.client import ClientError \n",
    "import configparser\n",
    "import psycopg2 \n",
    "import requests\n",
    "from AWS_S3_Functions import checkS3bucket, buildS3bucket, loadToS3\n",
    "\n",
    "\n",
    "# so we can explore all columns on any pandas DF printed out \n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## Step 1: Project Scope & Data Gather\n",
    "#### Scope \n",
    "The requirements of this project are to create an ETL pipeline which builds ready to use analytical tables for analysts within the Department for Transport in the UK Government. This is so analysts can produce MI & insights into national, regional & constituency road traffic accidents. It will also allow analysts to investigate the links between Driving Test centres, average income, local police forces, and other variables.\n",
    "\n",
    "##### Project Steps\n",
    "- Source Data from online sources. Download these data to local storage as part of the project folder.\n",
    "- Import each data source, explore, clean and export back to a new location for holding clean data files.\n",
    "- Create an S3 bucket on AWS, to store these newly cleaned files for use within an ETL pipeline. Large, cheap and easily accessible storage is available via AWS S3.\n",
    "- Next, a redshift cluster will be created, which will contain staging tables (loading the data from S3) before extracting, transforming and loading that data into a series of analytical tables (star schema format) which will be used by the analysts in the DoT of the UK Government. \n",
    "- Finally the staging tables are then dropped.\n",
    "- Some DQ queries are ran against the new database analytical tables, to check for DQ constraints and that tables have loaded as expected.\n",
    "- This project finally finishes by cleaning up the different resources created; removing the redshift cluster, the IAM role and the S3 objects & bucket.\n",
    "\n",
    "\n",
    "Key steps of the process are configured via the **\"AWS.cfg\"** file, in order to allow the pipeline to be easily modified for individual use, with personal details, removing the need to modify multiple lines of code.\n",
    "\n",
    "\n",
    "#### The Data Sources\n",
    "This project makes use of datasets that are sourced from Kaggle (though originating from gov.uk open source data) and the free to access website `www.doogal.co.uk` which provides UK Location data. Below will cover some more information for each data source to be used in this project. <br>\n",
    "The project provides code that will collect the data from the relevant web-pages and store locally in the project working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>** UK Road Safety: Traffic Accidents & Vehicles Data **</font>\n",
    "\n",
    "The following data is sourced via Kaggle. <br>\n",
    "The data originally come from the Open Data website of the UK Government, as detailed in the Kaggle description. <br> \n",
    "That site is linked here: <https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data> \n",
    "\n",
    "As described on the kaggle post:\n",
    "\n",
    "These datasets features a wide range of information regarding traffic accident in the UK, such as vehicle details, accident location, road details, weather details & much more\n",
    "- *Accident Information* : every line within the file represents a unique traffic accident (identified by the AccidentIndex column), featuring various properties related to the accident as columns. Date range: 2005-2017\n",
    "- *Vehicle Information* : every line in the file represents the involvement of a unique vehicle in a unique traffic accident, featuring various vehicle and passenger properties as columns. Date range: 2004-2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uk-road-safety-accidents-and-vehicles.zip to c:\\Users\\Dan\\Documents\\Work\\Python\\Learning Material\\4. Udacity - NanoDegree\\99. Capstone Final Project - my own idea\\Project Files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/142M [00:00<?, ?B/s]\n",
      "  1%|▏         | 2.00M/142M [00:00<00:11, 13.0MB/s]\n",
      "  3%|▎         | 4.00M/142M [00:00<00:11, 13.1MB/s]\n",
      "  4%|▍         | 6.00M/142M [00:00<00:11, 12.6MB/s]\n",
      "  6%|▌         | 8.00M/142M [00:00<00:10, 13.1MB/s]\n",
      "  7%|▋         | 10.0M/142M [00:00<00:10, 13.4MB/s]\n",
      "  8%|▊         | 12.0M/142M [00:00<00:10, 13.2MB/s]\n",
      " 10%|▉         | 14.0M/142M [00:01<00:10, 13.4MB/s]\n",
      " 11%|█         | 16.0M/142M [00:01<00:09, 13.7MB/s]\n",
      " 13%|█▎        | 18.0M/142M [00:01<00:09, 13.7MB/s]\n",
      " 14%|█▍        | 20.0M/142M [00:01<00:09, 13.9MB/s]\n",
      " 15%|█▌        | 22.0M/142M [00:01<00:09, 13.8MB/s]\n",
      " 17%|█▋        | 24.0M/142M [00:01<00:08, 13.9MB/s]\n",
      " 18%|█▊        | 26.0M/142M [00:02<00:08, 13.8MB/s]\n",
      " 20%|█▉        | 28.0M/142M [00:02<00:08, 13.9MB/s]\n",
      " 21%|██        | 30.0M/142M [00:02<00:08, 13.7MB/s]\n",
      " 22%|██▏       | 32.0M/142M [00:02<00:08, 13.6MB/s]\n",
      " 24%|██▍       | 34.0M/142M [00:02<00:08, 13.6MB/s]\n",
      " 25%|██▌       | 36.0M/142M [00:02<00:08, 13.7MB/s]\n",
      " 27%|██▋       | 38.0M/142M [00:02<00:07, 13.9MB/s]\n",
      " 28%|██▊       | 40.0M/142M [00:03<00:07, 13.9MB/s]\n",
      " 30%|██▉       | 42.0M/142M [00:03<00:07, 13.9MB/s]\n",
      " 31%|███       | 44.0M/142M [00:03<00:07, 13.7MB/s]\n",
      " 32%|███▏      | 46.0M/142M [00:03<00:07, 13.8MB/s]\n",
      " 34%|███▎      | 48.0M/142M [00:03<00:07, 13.7MB/s]\n",
      " 35%|███▌      | 50.0M/142M [00:03<00:06, 13.8MB/s]\n",
      " 37%|███▋      | 52.0M/142M [00:03<00:06, 13.9MB/s]\n",
      " 38%|███▊      | 54.0M/142M [00:04<00:06, 13.9MB/s]\n",
      " 39%|███▉      | 56.0M/142M [00:04<00:06, 13.9MB/s]\n",
      " 41%|████      | 58.0M/142M [00:04<00:06, 13.8MB/s]\n",
      " 42%|████▏     | 60.0M/142M [00:04<00:06, 13.6MB/s]\n",
      " 44%|████▎     | 62.0M/142M [00:04<00:06, 13.7MB/s]\n",
      " 45%|████▍     | 64.0M/142M [00:04<00:05, 13.7MB/s]\n",
      " 46%|████▋     | 66.0M/142M [00:05<00:06, 11.9MB/s]\n",
      " 48%|████▊     | 68.0M/142M [00:05<00:07, 11.1MB/s]\n",
      " 49%|████▉     | 70.0M/142M [00:05<00:06, 12.0MB/s]\n",
      " 51%|█████     | 72.0M/142M [00:05<00:05, 12.4MB/s]\n",
      " 52%|█████▏    | 74.0M/142M [00:05<00:05, 12.8MB/s]\n",
      " 53%|█████▎    | 76.0M/142M [00:05<00:05, 13.1MB/s]\n",
      " 55%|█████▍    | 78.0M/142M [00:06<00:05, 13.4MB/s]\n",
      " 56%|█████▌    | 80.0M/142M [00:06<00:04, 13.4MB/s]\n",
      " 58%|█████▊    | 82.0M/142M [00:06<00:04, 13.6MB/s]\n",
      " 59%|█████▉    | 84.0M/142M [00:06<00:04, 13.7MB/s]\n",
      " 60%|██████    | 86.0M/142M [00:06<00:04, 13.7MB/s]\n",
      " 62%|██████▏   | 88.0M/142M [00:06<00:04, 13.0MB/s]\n",
      " 63%|██████▎   | 90.0M/142M [00:07<00:04, 12.0MB/s]\n",
      " 65%|██████▍   | 92.0M/142M [00:07<00:04, 12.6MB/s]\n",
      " 66%|██████▌   | 94.0M/142M [00:07<00:03, 13.0MB/s]\n",
      " 67%|██████▋   | 96.0M/142M [00:07<00:03, 13.0MB/s]\n",
      " 69%|██████▉   | 98.0M/142M [00:07<00:03, 13.2MB/s]\n",
      " 70%|███████   | 100M/142M [00:07<00:03, 13.5MB/s] \n",
      " 72%|███████▏  | 102M/142M [00:08<00:03, 13.5MB/s]\n",
      " 73%|███████▎  | 104M/142M [00:08<00:02, 13.5MB/s]\n",
      " 75%|███████▍  | 106M/142M [00:08<00:02, 13.6MB/s]\n",
      " 76%|███████▌  | 108M/142M [00:08<00:02, 13.6MB/s]\n",
      " 77%|███████▋  | 110M/142M [00:08<00:02, 13.6MB/s]\n",
      " 79%|███████▊  | 112M/142M [00:08<00:02, 13.4MB/s]\n",
      " 80%|████████  | 114M/142M [00:08<00:02, 13.4MB/s]\n",
      " 82%|████████▏ | 116M/142M [00:09<00:02, 13.5MB/s]\n",
      " 83%|████████▎ | 118M/142M [00:09<00:01, 13.5MB/s]\n",
      " 84%|████████▍ | 120M/142M [00:09<00:01, 13.6MB/s]\n",
      " 86%|████████▌ | 122M/142M [00:09<00:01, 13.6MB/s]\n",
      " 87%|████████▋ | 124M/142M [00:09<00:01, 13.8MB/s]\n",
      " 89%|████████▊ | 126M/142M [00:09<00:01, 13.8MB/s]\n",
      " 90%|████████▉ | 128M/142M [00:10<00:01, 13.7MB/s]\n",
      " 91%|█████████▏| 130M/142M [00:10<00:00, 13.6MB/s]\n",
      " 93%|█████████▎| 132M/142M [00:10<00:01, 10.6MB/s]\n",
      " 94%|█████████▍| 134M/142M [00:10<00:00, 11.4MB/s]\n",
      " 96%|█████████▌| 136M/142M [00:10<00:00, 12.1MB/s]\n",
      " 97%|█████████▋| 138M/142M [00:10<00:00, 12.5MB/s]\n",
      " 98%|█████████▊| 140M/142M [00:11<00:00, 12.9MB/s]\n",
      "100%|█████████▉| 142M/142M [00:11<00:00, 13.2MB/s]\n",
      "100%|██████████| 142M/142M [00:11<00:00, 13.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the kaggle API to download both \"Accident\" & \"Vehicle\" CSV files - run via a command line call\n",
    "!kaggle datasets download -d tsiaras/uk-road-safety-accidents-and-vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the zip file that will have been downloaded, extract the contents\n",
    "accidents_location = r\"uk-road-safety-accidents-and-vehicles.zip\" \n",
    "\n",
    "with zipfile.ZipFile(accidents_location, 'r') as zip_ref:\n",
    "    zip_ref.extractall() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>** UK Driving Test Centres Data **</font>\n",
    "\n",
    "The following data is sourced via Kaggle. <br>\n",
    "The data originally come from the Open Data website of the UK Government, as detailed in the Kaggle description. <br> \n",
    "That site is linked here: <https://data.gov.uk/dataset/fe19beff-5716-4ca9-be58-027e56856b48/driving-test-centres> \n",
    "\n",
    "The original on the site, is in CSV format, however, the kaggle version has been created in JSON format.\n",
    "\n",
    "This dataset contains locations of Driving Test Centres in the UK. The test centre name, postcode and latitude & longitude co-ordinates are available in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uk-driving-test-centres.zip to c:\\Users\\Dan\\Documents\\Work\\Python\\Learning Material\\4. Udacity - NanoDegree\\99. Capstone Final Project - my own idea\\Project Files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/38.2k [00:00<?, ?B/s]\n",
      "100%|██████████| 38.2k/38.2k [00:00<00:00, 3.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "# download data to zipped file \n",
    "!kaggle datasets download -d dancollins3dmc/uk-driving-test-centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the zip file that will have been downloaded, extract the contents\n",
    "test_centre_location = r\"uk-driving-test-centres.zip\"  \n",
    "\n",
    "with zipfile.ZipFile(test_centre_location, 'r') as zip_ref:\n",
    "    zip_ref.extractall() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>** UK LSOA location data **</font>\n",
    "\n",
    "The following data is sourced via the website Doogal. <br>\n",
    "That site is linked here: <https://www.doogal.co.uk/postcodedownloads.php>\n",
    "\n",
    "These datasets contains key location data for UK Postcodes & LSOAs (Lower Layer Super Output Areas). <br> \n",
    "The files also contains data regarding political constituency, ditrict, ward, nearest train station, police force & average income. <br> \n",
    "\n",
    "Each Country is accessible via CSV format through a URL.\n",
    "\n",
    "- England : <https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=England>\n",
    "- Wales : <https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Wales>\n",
    "- Scotland : <https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Scotland>\n",
    "- Northern Ireland : <https://www.doogal.co.uk/UKPostcodesCSV.ashx?Search=BT> \n",
    "\n",
    "<br>\n",
    "This project will actually use a summarised version of these 4 postcode files, where the data will be grouped to the distinct LSOA level, and the Police Force, Avg Income & other high level location information retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=England</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Wales</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Scotland</td>\n",
       "      <td>Scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.doogal.co.uk/UKPostcodesCSV.ashx?Search=BT</td>\n",
       "      <td>Northern Ireland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             URL  \\\n",
       "0   https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=England   \n",
       "1     https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Wales   \n",
       "2  https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Scotland   \n",
       "3         https://www.doogal.co.uk/UKPostcodesCSV.ashx?Search=BT   \n",
       "\n",
       "            Country  \n",
       "0           England  \n",
       "1             Wales  \n",
       "2          Scotland  \n",
       "3  Northern Ireland  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download all 4 postcode files to working Directory\n",
    "urls = [r\"https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=England\",\n",
    "        r\"https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Wales\",   \n",
    "        r\"https://www.doogal.co.uk/UKPostcodesCSV.ashx?country=Scotland\",      \n",
    "        r\"https://www.doogal.co.uk/UKPostcodesCSV.ashx?Search=BT\"\n",
    "]\n",
    "countries = [\"England\", \"Wales\", \"Scotland\", \"Northern Ireland\"] \n",
    "filelists = pd.DataFrame.from_dict({'URL': urls, 'Country': countries})\n",
    "filelists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Writing England.txt\n",
      "Finished Writing England.txt\n",
      "Start Writing Wales.txt\n",
      "Finished Writing Wales.txt\n",
      "Start Writing Scotland.txt\n",
      "Finished Writing Scotland.txt\n",
      "Start Writing Northern Ireland.txt\n",
      "Finished Writing Northern Ireland.txt\n"
     ]
    }
   ],
   "source": [
    "# loop through each row, read chunks from the URL, download into a textfile locally \n",
    "for index, row in filelists.iterrows():\n",
    "    url = row['URL'] \n",
    "    response = requests.get(url, stream = True) \n",
    "    outfile = row['Country'] + \".txt\" \n",
    "    textfile = open(outfile, \"wb\")\n",
    "    print(\"Start Writing \" + outfile)\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        textfile.write(chunk)\n",
    "\n",
    "    print(\"Finished Writing \" + outfile)\n",
    "    textfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file in (via chunks to reduce out of memory errors) \n",
    "# columns to retain \n",
    "lsoa_cols = [\"LSOA Code\", \"Country\", \"Constituency\", \"Average Income\", \"Police force\"] \n",
    "# list of files downloaded \n",
    "txt_list = [\"England.txt\", \"Scotland.txt\", \"Wales.txt\", \"Northern Ireland.txt\"] \n",
    "# empty holder list \n",
    "all_files = [] \n",
    "\n",
    "for txt in txt_list:\n",
    "    for chunk in pd.read_csv(txt, usecols=lsoa_cols, chunksize=10000):\n",
    "        all_files.append(chunk) \n",
    "\n",
    "lsoa_data = pd.concat(all_files, axis=0) # appends all smaller dataframes into one larger one (vertically) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data back to one CSV file \n",
    "lsoa_data.to_csv(\"LSOA_Data.csv\", sep='~', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up no longer required separate files\n",
    "for txt in txt_list:\n",
    "    os.remove(txt)\n",
    "# end     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some clean-up \n",
    "del lsoa_data, all_files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Step 2: Explore & Assess Data\n",
    "#### Data Exploration - UK Driving Test Centres\n",
    "\n",
    "Start by looking at the `UK Driving Test Centres` JSON file. Read in and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Address line 1</th>\n",
       "      <th>Address line 2</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Toilets</th>\n",
       "      <th>Disabled access</th>\n",
       "      <th>Parking</th>\n",
       "      <th>Other notes</th>\n",
       "      <th>Car</th>\n",
       "      <th>Motorcycle Module 1</th>\n",
       "      <th>Motorcycle module 2</th>\n",
       "      <th>LBCCT</th>\n",
       "      <th>B+E</th>\n",
       "      <th>Taxi</th>\n",
       "      <th>ADI Part 2</th>\n",
       "      <th>ADI Part 3</th>\n",
       "      <th>TARSID</th>\n",
       "      <th>Closing date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aberdeen North</td>\n",
       "      <td>Cloverhill Road</td>\n",
       "      <td>Bridge of Don</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Aberdeenshire</td>\n",
       "      <td>AB23 8FE</td>\n",
       "      <td>57.185055</td>\n",
       "      <td>-2.095251</td>\n",
       "      <td>Male, female and disabled toilets are available</td>\n",
       "      <td>You can get into the test centre in a wheelchair</td>\n",
       "      <td>No car park is available at the test centre</td>\n",
       "      <td>For Car Tests: Please Keep to the one way system around the building and meet your examiner at the VIC waiting room on left of the DVSA building. Please also note that no practising is allowed on site.</td>\n",
       "      <td>Car</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Voc</td>\n",
       "      <td>B+E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADI2</td>\n",
       "      <td>ADI3</td>\n",
       "      <td>337.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberdeen South (Cove)</td>\n",
       "      <td>Moss Road</td>\n",
       "      <td>Gateway Business Park, Nigg</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Aberdeenshire</td>\n",
       "      <td>AB12 3GQ</td>\n",
       "      <td>57.088588</td>\n",
       "      <td>-2.107476</td>\n",
       "      <td>Male, female and disabled toilets are available</td>\n",
       "      <td>You can get into the test centre in a wheelchair</td>\n",
       "      <td>Car parking spaces are available</td>\n",
       "      <td>None</td>\n",
       "      <td>Car</td>\n",
       "      <td>Mc1</td>\n",
       "      <td>Mc2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>ADI3</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>Town Hall</td>\n",
       "      <td>Crieff Road</td>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>Perth and Kinross</td>\n",
       "      <td>PH15 2BJ</td>\n",
       "      <td>56.618039</td>\n",
       "      <td>-3.868265</td>\n",
       "      <td>Male, female and disabled toilets are available</td>\n",
       "      <td>You can get into the test centre in a wheelchair</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Car</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>247.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name   Address line 1                 Address line 2  \\\n",
       "0         Aberdeen North  Cloverhill Road                Bridge of Don     \n",
       "1  Aberdeen South (Cove)        Moss Road  Gateway Business Park, Nigg     \n",
       "2              Aberfeldy        Town Hall                 Crieff Road      \n",
       "\n",
       "        City             County  Postcode        Lat      Long  \\\n",
       "0   Aberdeen      Aberdeenshire  AB23 8FE  57.185055 -2.095251   \n",
       "1   Aberdeen      Aberdeenshire  AB12 3GQ  57.088588 -2.107476   \n",
       "2  Aberfeldy  Perth and Kinross  PH15 2BJ  56.618039 -3.868265   \n",
       "\n",
       "                                           Toilets  \\\n",
       "0  Male, female and disabled toilets are available   \n",
       "1  Male, female and disabled toilets are available   \n",
       "2  Male, female and disabled toilets are available   \n",
       "\n",
       "                                    Disabled access  \\\n",
       "0  You can get into the test centre in a wheelchair   \n",
       "1  You can get into the test centre in a wheelchair   \n",
       "2  You can get into the test centre in a wheelchair   \n",
       "\n",
       "                                        Parking  \\\n",
       "0  No car park is available at the test centre    \n",
       "1              Car parking spaces are available   \n",
       "2                                          None   \n",
       "\n",
       "                                                                                                                                                                                                 Other notes  \\\n",
       "0  For Car Tests: Please Keep to the one way system around the building and meet your examiner at the VIC waiting room on left of the DVSA building. Please also note that no practising is allowed on site.   \n",
       "1                                                                                                                                                                                                       None   \n",
       "2                                                                                                                                                                                                       None   \n",
       "\n",
       "   Car Motorcycle Module 1 Motorcycle module 2 LBCCT   B+E  Taxi ADI Part 2  \\\n",
       "0  Car                None                None   Voc   B+E   NaN       ADI2   \n",
       "1  Car                 Mc1                 Mc2  None  None   NaN       None   \n",
       "2  Car                None                None  None  None   NaN       None   \n",
       "\n",
       "  ADI Part 3  TARSID  Closing date  \n",
       "0       ADI3   337.0           NaN  \n",
       "1       ADI3  1878.0           NaN  \n",
       "2       None   247.0           NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_centres = pd.read_json(r\"DriveTest_Centres.json\") \n",
    "test_centres.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only some variables are of interest for the database being built\n",
    "Variables of interest are: <br>\n",
    "- `Name, City, County, Postcode, Lat, Long, Closing date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Centre file has 362 rows\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows in the dataset\n",
    "print(\"Test Centre file has \" + str(len(test_centres)) + \" rows\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name              2\n",
       "City              6\n",
       "County            2\n",
       "Postcode          2\n",
       "Lat               2\n",
       "Long              2\n",
       "Closing date    362\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for nulls in these key columns \n",
    "tc_cols = [\"Name\", \"City\", \"County\", \"Postcode\", \"Lat\", \"Long\", \"Closing date\"]\n",
    "display(test_centres[tc_cols].isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the results that, two of the values in the `Name` column are null. These will be dropped from the table. <br>\n",
    "There also appears to be two nulls within Latitude & Longitude, which are key variables for furture joins. This would need to be dropped also. Let's check if they are all the same cases. <br>\n",
    "We can also see all `Closing date` values are NULL, so no test centres are, or due to be, closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Closing date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  City County Postcode  Lat  Long  Closing date\n",
       "360  None  None   None     None  NaN   NaN           NaN\n",
       "361  None  None   None     None  NaN   NaN           NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = test_centres[tc_cols] \n",
    "tc.loc[tc['Name'].isnull()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like all these cases are indeed linked. As such, these will need to be dropped.\n",
    "- Cleaning Note 1: Drop cases where `Name` is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Closing date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  City County Postcode  Lat  Long  Closing date\n",
       "360  None  None   None     None  NaN   NaN           NaN\n",
       "361  None  None   None     None  NaN   NaN           NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for & return any duplicate rows \n",
    "tc[tc.duplicated(['Name'], keep=False)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. No duplicated `Names` other than the NULLs of which we were aware. <br>\n",
    "Let's take a look at cases where `City` was NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Closing date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Barry</td>\n",
       "      <td>None</td>\n",
       "      <td>South Glamorgan</td>\n",
       "      <td>CF62 5QN</td>\n",
       "      <td>51.399694</td>\n",
       "      <td>-3.279244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Goodmayes (London)</td>\n",
       "      <td>None</td>\n",
       "      <td>Greater London</td>\n",
       "      <td>IG3 9UZ</td>\n",
       "      <td>51.563820</td>\n",
       "      <td>0.110019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Letchworth</td>\n",
       "      <td>None</td>\n",
       "      <td>Hertfordshire</td>\n",
       "      <td>SG6 1RF</td>\n",
       "      <td>51.978204</td>\n",
       "      <td>-0.214656</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Orkney</td>\n",
       "      <td>None</td>\n",
       "      <td>Orkney</td>\n",
       "      <td>KW15 1FL</td>\n",
       "      <td>58.981674</td>\n",
       "      <td>-2.972001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name  City           County  Postcode        Lat      Long  \\\n",
       "27                Barry  None  South Glamorgan  CF62 5QN  51.399694 -3.279244   \n",
       "145  Goodmayes (London)  None   Greater London   IG3 9UZ  51.563820  0.110019   \n",
       "208          Letchworth  None    Hertfordshire   SG6 1RF  51.978204 -0.214656   \n",
       "253             Orkney   None           Orkney  KW15 1FL  58.981674 -2.972001   \n",
       "360                None  None             None      None        NaN       NaN   \n",
       "361                None  None             None      None        NaN       NaN   \n",
       "\n",
       "     Closing date  \n",
       "27            NaN  \n",
       "145           NaN  \n",
       "208           NaN  \n",
       "253           NaN  \n",
       "360           NaN  \n",
       "361           NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.loc[tc['City'].isnull()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, except for the cases of NULL, we have 4 cases where `City` is empty. \n",
    "For these cases, we will clean them by filling the empty column ith the `County`. <br>\n",
    "\n",
    "#### Data Clean - UK Driving Test Centres\n",
    "- Remove cases where `Name` is NULL\n",
    "- Fill `City` blanks with `County` column values\n",
    "\n",
    "Write cleaned dataset back out to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after NULL drops: 360\n",
      "Number of NULL `City` values: 0\n"
     ]
    }
   ],
   "source": [
    "tc_clean = tc.loc[tc['Name'].notnull()] \n",
    "print(\"Number of records after NULL drops: \" +str(len(tc_clean))) \n",
    "# we should now see an output of 360 rows, 2 having been dropped \n",
    "\n",
    "# next, clean `City` column to copy over the `County` value \n",
    "tc_clean = tc_clean.copy() \n",
    "tc_clean.loc[tc_clean['City'].isnull(), 'City'] = tc_clean['County'] \n",
    "# re-run check for nulls in this column, should now be zero \n",
    "check_city = tc_clean.loc[tc_clean['City'].isnull()] \n",
    "print(\"Number of NULL `City` values: \" + str(len(check_city))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the \"clean_data\" directory exists, if not, create it \n",
    "if not os.path.exists(\"clean_data\"):\n",
    "    os.makedirs(\"clean_data\")\n",
    "\n",
    "# write clean data back to clean file \n",
    "tc_clean.to_json(\"clean_data/UK_Driving_Test_Centre.json\", orient='records', lines=True) \n",
    "\n",
    "del tc_clean, tc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration - UK LSOA Data\n",
    "\n",
    "Read in and explore the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Constituency</th>\n",
       "      <th>LSOA Code</th>\n",
       "      <th>Police force</th>\n",
       "      <th>Average Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England</td>\n",
       "      <td>St Albans</td>\n",
       "      <td>E01023743</td>\n",
       "      <td>Hertfordshire</td>\n",
       "      <td>68900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>St Albans</td>\n",
       "      <td>E01023667</td>\n",
       "      <td>Hertfordshire</td>\n",
       "      <td>66500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>St Albans</td>\n",
       "      <td>E01023667</td>\n",
       "      <td>Hertfordshire</td>\n",
       "      <td>66500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country Constituency  LSOA Code   Police force  Average Income\n",
       "0  England    St Albans  E01023743  Hertfordshire         68900.0\n",
       "1  England    St Albans  E01023667  Hertfordshire         66500.0\n",
       "2  England    St Albans  E01023667  Hertfordshire         66500.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoa = pd.read_csv(\"LSOA_Data.csv\", sep='~', usecols=lsoa_cols) \n",
    "lsoa.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSOA has 2652196 rows\n",
      "** ------------------------------ **\n",
      "Country has 4 unique values\n",
      "Constituency has 650 unique values\n",
      "LSOA Code has 42619 unique values\n",
      "Police force has 45 unique values\n",
      "Average Income has 532 unique values\n"
     ]
    }
   ],
   "source": [
    "print(\"LSOA has \" + str(len(lsoa)) + \" rows\") \n",
    "print(\"** ------------------------------ **\")\n",
    "lsoa_cols = lsoa.columns.values.tolist() \n",
    "for col in lsoa_cols:\n",
    "    unique_count = lsoa[col].nunique() \n",
    "    print(col + \" has \" + str(unique_count) + \" unique values\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic checks confirm good DQ here. 4 countries, as does make up the United Kingdom, so we would expect to see in the data built here from our source. Likewise, the UK has 650 seats in the house of commons (UK parliament) which matches to the 650 `Constituency` count we can see here.\n",
    "\n",
    "Also, as we can see `LSOA Code` has the highest frequency of distinct values. This was expected as LSOA is a quite granular location variable, used by the UK ONS for reporting small area statistics. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2652196 entries, 0 to 2652195\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Country         object \n",
      " 1   Constituency    object \n",
      " 2   LSOA Code       object \n",
      " 3   Police force    object \n",
      " 4   Average Income  float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 101.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# check info() \n",
    "lsoa.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in Country is: 0\n",
      "Null count in Constituency is: 10332\n",
      "Null count in LSOA Code is: 10332\n",
      "Null count in Police force is: 10332\n",
      "Null count in Average Income is: 295946\n"
     ]
    }
   ],
   "source": [
    "# search for instances of LSOA Code being null - these will need to be removed as part of the cleaning\n",
    "for col in lsoa_cols:\n",
    "    null_count = lsoa.loc[lsoa[col].isnull()]\n",
    "    print(\"Null count in \" + col + \" is: \"+ str(len(null_count))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Clean - UK LSOA location data\n",
    "\n",
    "By removing the null LSOA rows, we will also clean the District, Constituency & Police Force columns. <br> \n",
    "For missing Average Income, we will use the latest UK ONS **Median** Household Income value, as a proxy for average income. \n",
    "\n",
    "Cleaning Actions:\n",
    "- remove the 10,332 rows with NULL `LSOA Code` \n",
    "- where `Average Income` is missing, replace with UK 2020 Median Income value according to the ONS (£29,900)  <https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyear2020> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in Country is: 0\n",
      "Null count in Constituency is: 0\n",
      "Null count in LSOA Code is: 0\n",
      "Null count in Police force is: 0\n",
      "Null count in Average Income is: 0\n"
     ]
    }
   ],
   "source": [
    "# replace NULLs in Avg Income with 29,900 \n",
    "lsoa['Average Income'] = lsoa['Average Income'].fillna(29900) \n",
    "\n",
    "# drop rows where LSOA Code is NULL \n",
    "lsoa.dropna(subset=['LSOA Code'], axis=0, inplace=True)  # axis=0 specifies rows containing 0 dropped as per Pandas API docs\n",
    "\n",
    "# check NULLs again \n",
    "for col in lsoa.columns.values.tolist():\n",
    "    null_count = lsoa.loc[lsoa[col].isnull()]\n",
    "    print(\"Null count in \" + col + \" is: \"+ str(len(null_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any NULL values. <br>\n",
    "The final stage is to summarise with a `groupby()` to remoave the duplication at LSOA Code level, taking the mean of the `Average Income` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Constituency</th>\n",
       "      <th>LSOA Code</th>\n",
       "      <th>Police force</th>\n",
       "      <th>Average Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33143</th>\n",
       "      <td>England</td>\n",
       "      <td>Wyre Forest</td>\n",
       "      <td>E01032469</td>\n",
       "      <td>West Mercia</td>\n",
       "      <td>31200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17772</th>\n",
       "      <td>England</td>\n",
       "      <td>Manchester, Gorton</td>\n",
       "      <td>E01005280</td>\n",
       "      <td>Greater Manchester</td>\n",
       "      <td>30900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>England</td>\n",
       "      <td>Boston and Skegness</td>\n",
       "      <td>E01026013</td>\n",
       "      <td>Lincolnshire</td>\n",
       "      <td>41300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country         Constituency  LSOA Code        Police force  \\\n",
       "33143  England          Wyre Forest  E01032469         West Mercia   \n",
       "17772  England   Manchester, Gorton  E01005280  Greater Manchester   \n",
       "3353   England  Boston and Skegness  E01026013        Lincolnshire   \n",
       "\n",
       "       Average Income  \n",
       "33143         31200.0  \n",
       "17772         30900.0  \n",
       "3353          41300.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupCols = [\"Country\", \"Constituency\", \"LSOA Code\", \"Police force\"] \n",
    "lsoa_clean = lsoa.groupby(groupCols).mean().reset_index() \n",
    "lsoa_clean.sample(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate rows in lsoa_clean is: 0\n"
     ]
    }
   ],
   "source": [
    "# check the number of duplicate rows in dataset - should be 0\n",
    "print(\"The number of duplicate rows in lsoa_clean is: \" + str(len(lsoa_clean[lsoa_clean.duplicated()]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that no duplicates exist at the LSOA & Constituency level (which will act as Primary Keys) - Answer should be `False`\n",
    "lsoa_clean[['LSOA Code','Constituency']].duplicated().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write clean data back to CSV file in clean_data folder \n",
    "# check if the \"clean_data\" directory exists, if not, create it \n",
    "if not os.path.exists(\"clean_data\"):\n",
    "    os.makedirs(\"clean_data\")\n",
    "\n",
    "# write clean data back to clean file \n",
    "lsoa_clean.to_csv(\"clean_data/UK_LSOA_Data.csv\", sep='~', index=False) \n",
    "\n",
    "del lsoa_clean, lsoa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration - UK Accidents Information\n",
    "\n",
    "Read in and explore the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>1st_Road_Class</th>\n",
       "      <th>1st_Road_Number</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>LSOA_of_Accident_Location</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Number_of_Vehicles</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>Road_Type</th>\n",
       "      <th>Speed_limit</th>\n",
       "      <th>Time</th>\n",
       "      <th>Weather_Conditions</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1407594</th>\n",
       "      <td>2013160B02691</td>\n",
       "      <td>A</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2013-09-02</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.590495</td>\n",
       "      <td>-0.391215</td>\n",
       "      <td>E01013270</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Dual carriageway</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15:55</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635882</th>\n",
       "      <td>2014950005820</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.928875</td>\n",
       "      <td>-3.022169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>Dual carriageway</td>\n",
       "      <td>70.0</td>\n",
       "      <td>17:30</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375650</th>\n",
       "      <td>2006920600152</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2006-01-11</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.161759</td>\n",
       "      <td>-2.148266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17:45</td>\n",
       "      <td>Fine + high winds</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accident_Index 1st_Road_Class  1st_Road_Number Accident_Severity  \\\n",
       "1407594  2013160B02691              A            180.0           Serious   \n",
       "1635882  2014950005820              A              1.0            Slight   \n",
       "375650   2006920600152   Unclassified              0.0           Serious   \n",
       "\n",
       "               Date Day_of_Week  Did_Police_Officer_Attend_Scene_of_Accident  \\\n",
       "1407594  2013-09-02      Monday                                          1.0   \n",
       "1635882  2014-11-11     Tuesday                                          1.0   \n",
       "375650   2006-01-11   Wednesday                                          1.0   \n",
       "\n",
       "          Latitude  Longitude LSOA_of_Accident_Location  Number_of_Casualties  \\\n",
       "1407594  53.590495  -0.391215                 E01013270                     3   \n",
       "1635882  55.928875  -3.022169                       NaN                     1   \n",
       "375650   57.161759  -2.148266                       NaN                     1   \n",
       "\n",
       "         Number_of_Vehicles Road_Surface_Conditions           Road_Type  \\\n",
       "1407594                   1                     Dry    Dual carriageway   \n",
       "1635882                   2             Wet or damp    Dual carriageway   \n",
       "375650                    1                     Dry  Single carriageway   \n",
       "\n",
       "         Speed_limit   Time  Weather_Conditions  Year  \n",
       "1407594         70.0  15:55  Fine no high winds  2013  \n",
       "1635882         70.0  17:30  Fine no high winds  2014  \n",
       "375650          30.0  17:45   Fine + high winds  2006  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of the key columns of interest \n",
    "accident_cols = [\"Accident_Index\", \"1st_Road_Class\", \"1st_Road_Number\", \"Accident_Severity\", \"Date\", \"Day_of_Week\", \n",
    "                  \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Latitude\", \"Longitude\", \"LSOA_of_Accident_Location\",\n",
    "                  \"Number_of_Casualties\", \"Number_of_Vehicles\", \"Road_Surface_Conditions\", \"Road_Type\", \"Speed_limit\",\n",
    "                  \"Time\", \"Year\", \"Weather_Conditions\" \n",
    "] \n",
    "acc_files = [] # empty list to store chunk read-ins\n",
    "for chunk in pd.read_csv(\"Accident_Information.csv\", usecols=accident_cols, chunksize=10000):\n",
    "    acc_files.append(chunk) \n",
    "    \n",
    "\n",
    "accidents = pd.concat(acc_files, axis=0) \n",
    "accidents.sample(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2047256 entries, 0 to 2047255\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                       Dtype  \n",
      "---  ------                                       -----  \n",
      " 0   Accident_Index                               object \n",
      " 1   1st_Road_Class                               object \n",
      " 2   1st_Road_Number                              float64\n",
      " 3   Accident_Severity                            object \n",
      " 4   Date                                         object \n",
      " 5   Day_of_Week                                  object \n",
      " 6   Did_Police_Officer_Attend_Scene_of_Accident  float64\n",
      " 7   Latitude                                     float64\n",
      " 8   Longitude                                    float64\n",
      " 9   LSOA_of_Accident_Location                    object \n",
      " 10  Number_of_Casualties                         int64  \n",
      " 11  Number_of_Vehicles                           int64  \n",
      " 12  Road_Surface_Conditions                      object \n",
      " 13  Road_Type                                    object \n",
      " 14  Speed_limit                                  float64\n",
      " 15  Time                                         object \n",
      " 16  Weather_Conditions                           object \n",
      " 17  Year                                         int64  \n",
      "dtypes: float64(5), int64(3), object(10)\n",
      "memory usage: 281.1+ MB\n"
     ]
    }
   ],
   "source": [
    "accidents.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents has 2047256 rows\n",
      "** ------------------------------------------ **\n",
      "Null count in Accident_Index is: 0\n",
      "Null count in 1st_Road_Class is: 0\n",
      "Null count in 1st_Road_Number is: 2\n",
      "Null count in Accident_Severity is: 0\n",
      "Null count in Date is: 0\n",
      "Null count in Day_of_Week is: 0\n",
      "Null count in Did_Police_Officer_Attend_Scene_of_Accident is: 278\n",
      "Null count in Latitude is: 174\n",
      "Null count in Longitude is: 175\n",
      "Null count in LSOA_of_Accident_Location is: 144953\n",
      "Null count in Number_of_Casualties is: 0\n",
      "Null count in Number_of_Vehicles is: 0\n",
      "Null count in Road_Surface_Conditions is: 0\n",
      "Null count in Road_Type is: 0\n",
      "Null count in Speed_limit is: 37\n",
      "Null count in Time is: 156\n",
      "Null count in Year is: 0\n",
      "Null count in Weather_Conditions is: 0\n"
     ]
    }
   ],
   "source": [
    "# count rows of data\n",
    "print(\"Accidents has \" + str(len(accidents)) + \" rows\") \n",
    "\n",
    "print(\"** ------------------------------------------ **\")\n",
    "\n",
    "# check for NULLs in each column \n",
    "for col in accident_cols:\n",
    "    null_count = accidents.loc[accidents[col].isnull()]\n",
    "    print(\"Null count in \" + col + \" is: \"+ str(len(null_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial notes for cleaning:\n",
    "- drop all rows where the `LSOA_of_Accident_Location` is NULL\n",
    "- drop all rows where the `Latitude` or `Longitude` are NULL <br> \n",
    "        *This is because these fields are key in order to join additional reference data that will be provided in the database for analysis*  <br>\n",
    "<br> \n",
    "- where `Time` is NULL, set a default value of midnight (00:00) \n",
    "- where `Did_Police_Officer_Attend_Scene_of_Accident` is NULL, default to \"NO\" (value of 2) & clean the column to remove 1/0 and replace with descriptive Y or N \n",
    "- where `Speed_limit` is null, replace with default value? Maybe national speed limit of 60MPH\n",
    "- retain `Date` field, can drop `Year` & `Day_of_Week` as these will be derived from `Date` during table definitions in DB\n",
    "- create a `Timestamp` variable that uses both `Date` & `Time`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A               926729\n",
       "Unclassified    603938\n",
       "B               258076\n",
       "C               174953\n",
       "Motorway         78071\n",
       "A(M)              5489\n",
       "Name: 1st_Road_Class, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents[\"1st_Road_Class\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1638195\n",
       "2.0     403424\n",
       "3.0       5359\n",
       "Name: Did_Police_Officer_Attend_Scene_of_Accident, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents[\"Did_Police_Officer_Attend_Scene_of_Accident\"].value_counts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Cleaning Notes:\n",
    "- Clean the `A(M)` category into just `A` for Road Class\n",
    "- rename variables to just `Road_Class` and `Road_Number` \n",
    "- Re-classify the Police Attendance into a new variable \n",
    "\n",
    "\n",
    "#### Data Clean - UK Accidents Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where LSOA, Lat or Long are NULL\n",
    "accidents.dropna(subset=[\"Latitude\", \"Longitude\", \"LSOA_of_Accident_Location\"], axis=0, inplace=True) \n",
    "\n",
    "# fill in NULL times with 00:00\n",
    "accidents['Time'] = accidents['Time'].fillna(\"00:00\") \n",
    "\n",
    "# fill in NULL speed limits with 60MPH (UK National Speed Limit) \n",
    "accidents['Speed_limit'] = accidents['Speed_limit'].fillna(60) \n",
    "\n",
    "# clean missing police officer attend values to default NO (aka 2)\n",
    "accidents[\"Did_Police_Officer_Attend_Scene_of_Accident\"] = accidents[\"Did_Police_Officer_Attend_Scene_of_Accident\"].fillna(2) \n",
    "# NEW column, providing cleaner descriptive values for police attendance \n",
    "def police_attend(row):\n",
    "    if row[\"Did_Police_Officer_Attend_Scene_of_Accident\"] == 1 :\n",
    "        return 'Yes' \n",
    "    elif row[\"Did_Police_Officer_Attend_Scene_of_Accident\"] == 2 :\n",
    "        return 'No'\n",
    "    elif row[\"Did_Police_Officer_Attend_Scene_of_Accident\"] == 3 :\n",
    "        return 'No - Accident self reported via form' \n",
    "\n",
    "\n",
    "accidents[\"Police_Attended\"] = accidents.apply(lambda row: police_attend(row), axis=1) \n",
    "\n",
    "# drop columns Year & Day_of_Week & OLD Did Police Officer attend columns \n",
    "accidents.drop(columns=[\"Year\", \"Day_of_Week\", \"Did_Police_Officer_Attend_Scene_of_Accident\"], axis=0, inplace=True) \n",
    "\n",
    "# replace A(M) with just A in Road Class \n",
    "accidents[\"1st_Road_Class\"].replace({\"A(M)\": \"A\"}, inplace=True) \n",
    "\n",
    "# rename columns \n",
    "accidents.rename(columns={\"1st_Road_Class\": \"Road_Class\", \"1st_Road_Number\": \"Road_Number\"}, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1902289 entries, 0 to 2040142\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   Accident_Index             object \n",
      " 1   Road_Class                 object \n",
      " 2   Road_Number                float64\n",
      " 3   Accident_Severity          object \n",
      " 4   Date                       object \n",
      " 5   Latitude                   float64\n",
      " 6   Longitude                  float64\n",
      " 7   LSOA_of_Accident_Location  object \n",
      " 8   Number_of_Casualties       int64  \n",
      " 9   Number_of_Vehicles         int64  \n",
      " 10  Road_Surface_Conditions    object \n",
      " 11  Road_Type                  object \n",
      " 12  Speed_limit                float64\n",
      " 13  Time                       object \n",
      " 14  Weather_Conditions         object \n",
      " 15  Police_Attended            object \n",
      "dtypes: float64(4), int64(2), object(10)\n",
      "memory usage: 246.7+ MB\n"
     ]
    }
   ],
   "source": [
    "accidents.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in Accident_Index is: 0\n",
      "Null count in Road_Class is: 0\n",
      "Null count in Road_Number is: 0\n",
      "Null count in Accident_Severity is: 0\n",
      "Null count in Date is: 0\n",
      "Null count in Latitude is: 0\n",
      "Null count in Longitude is: 0\n",
      "Null count in LSOA_of_Accident_Location is: 0\n",
      "Null count in Number_of_Casualties is: 0\n",
      "Null count in Number_of_Vehicles is: 0\n",
      "Null count in Road_Surface_Conditions is: 0\n",
      "Null count in Road_Type is: 0\n",
      "Null count in Speed_limit is: 0\n",
      "Null count in Time is: 0\n",
      "Null count in Weather_Conditions is: 0\n",
      "Null count in Police_Attended is: 0\n"
     ]
    }
   ],
   "source": [
    "new_accident_cols = accidents.columns.values.tolist() \n",
    "# check for NULLs in each column \n",
    "for col in new_accident_cols:\n",
    "    null_count = accidents.loc[accidents[col].isnull()]\n",
    "    print(\"Null count in \" + col + \" is: \"+ str(len(null_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any NULLs, and now have all the variable names required. <br>\n",
    "Write the cleaned dataset to a new CSV file in the clean_data folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write clean data back to CSV file in clean_data folder \n",
    "# check if the \"clean_data\" directory exists, if not, create it \n",
    "if not os.path.exists(\"clean_data\"):\n",
    "    os.makedirs(\"clean_data\")\n",
    "\n",
    "# write clean data back to clean file \n",
    "accidents.to_csv(\"clean_data/UK_Accidents_Information.csv\", sep='~', index=False) \n",
    "\n",
    "del accidents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration - UK Vehicles Information\n",
    "\n",
    "Read in and explore the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Age_Band_of_Driver</th>\n",
       "      <th>Age_of_Vehicle</th>\n",
       "      <th>Engine_Capacity_.CC.</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>Sex_of_Driver</th>\n",
       "      <th>Vehicle_Manoeuvre</th>\n",
       "      <th>Vehicle_Type</th>\n",
       "      <th>X1st_Point_of_Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>832558</th>\n",
       "      <td>2010521005771</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>VOLVO</td>\n",
       "      <td>S60 T S AUTO</td>\n",
       "      <td>Not known</td>\n",
       "      <td>Turning right</td>\n",
       "      <td>Car</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622690</th>\n",
       "      <td>2014460238038</td>\n",
       "      <td>46 - 55</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>FIESTA FREESTYLE</td>\n",
       "      <td>Female</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>Car</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434246</th>\n",
       "      <td>2013440357636</td>\n",
       "      <td>36 - 45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>HYUNDAI</td>\n",
       "      <td>I20 COMFORT</td>\n",
       "      <td>Female</td>\n",
       "      <td>Moving off</td>\n",
       "      <td>Car</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accident_Index Age_Band_of_Driver  Age_of_Vehicle  \\\n",
       "832558   2010521005771            26 - 35             9.0   \n",
       "1622690  2014460238038            46 - 55            13.0   \n",
       "1434246  2013440357636            36 - 45             2.0   \n",
       "\n",
       "         Engine_Capacity_.CC.     make             model Sex_of_Driver  \\\n",
       "832558                 1984.0    VOLVO      S60 T S AUTO     Not known   \n",
       "1622690                1242.0     FORD  FIESTA FREESTYLE        Female   \n",
       "1434246                1248.0  HYUNDAI       I20 COMFORT        Female   \n",
       "\n",
       "         Vehicle_Manoeuvre Vehicle_Type X1st_Point_of_Impact  \n",
       "832558       Turning right          Car                Front  \n",
       "1622690  Going ahead other          Car                Front  \n",
       "1434246         Moving off          Car                Front  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of the key columns of interest \n",
    "vehicle_cols = [\n",
    "    \"Accident_Index\", \"Age_Band_of_Driver\", \"Age_of_Vehicle\", \"make\", \"model\", \"Engine_Capacity_.CC.\",\n",
    "    \"Sex_of_Driver\", \"Vehicle_Type\", \"Vehicle_Manoeuvre\", \"X1st_Point_of_Impact\"\n",
    "]\n",
    "veh_files = [] # empty list to store chunk read-ins\n",
    "# change encoding as utf-8 throws an error \n",
    "for chunk in pd.read_csv(\"Vehicle_Information.csv\", sep=',', encoding=\"ISO-8859-1\", usecols=vehicle_cols, chunksize=10000):\n",
    "    veh_files.append(chunk) \n",
    "    \n",
    "\n",
    "vehicles = pd.concat(veh_files, axis=0) \n",
    "vehicles.sample(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2177205 entries, 0 to 2177204\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   Accident_Index        object \n",
      " 1   Age_Band_of_Driver    object \n",
      " 2   Age_of_Vehicle        float64\n",
      " 3   Engine_Capacity_.CC.  float64\n",
      " 4   make                  object \n",
      " 5   model                 object \n",
      " 6   Sex_of_Driver         object \n",
      " 7   Vehicle_Manoeuvre     object \n",
      " 8   Vehicle_Type          object \n",
      " 9   X1st_Point_of_Impact  object \n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 166.1+ MB\n"
     ]
    }
   ],
   "source": [
    "vehicles.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in Accident_Index is: 0\n",
      "Null count in Age_Band_of_Driver is: 0\n",
      "Null count in Age_of_Vehicle is: 358149\n",
      "Null count in make is: 110845\n",
      "Null count in model is: 325331\n",
      "Null count in Engine_Capacity_.CC. is: 265861\n",
      "Null count in Sex_of_Driver is: 0\n",
      "Null count in Vehicle_Type is: 0\n",
      "Null count in Vehicle_Manoeuvre is: 0\n",
      "Null count in X1st_Point_of_Impact is: 0\n"
     ]
    }
   ],
   "source": [
    "# check for NULLs in each column \n",
    "for col in vehicle_cols:\n",
    "    null_count = vehicles.loc[vehicles[col].isnull()]\n",
    "    print(\"Null count in \" + col + \" is: \"+ str(len(null_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any duplicates within the Accident_Index \n",
    "dupe_check = vehicles['Accident_Index'].duplicated().any() \n",
    "dupe_check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that Accident Index is indeed duplicated within the vehicles data. This is likely as we multiple vehicles involved in accidents as times. <br> \n",
    "Let's check assumption by finding a duplicate `Accident_Index` record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Age_Band_of_Driver</th>\n",
       "      <th>Age_of_Vehicle</th>\n",
       "      <th>Engine_Capacity_.CC.</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>Sex_of_Driver</th>\n",
       "      <th>Vehicle_Manoeuvre</th>\n",
       "      <th>Vehicle_Type</th>\n",
       "      <th>X1st_Point_of_Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200401BS00003</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>NISSAN</td>\n",
       "      <td>MICRA CELEBRATION 16V</td>\n",
       "      <td>Male</td>\n",
       "      <td>Turning right</td>\n",
       "      <td>109</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200401BS00003</td>\n",
       "      <td>66 - 75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LONDON TAXIS INT</td>\n",
       "      <td>TXII GOLD AUTO</td>\n",
       "      <td>Male</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>109</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200401BS00004</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>PIAGGIO</td>\n",
       "      <td>VESPA ET4</td>\n",
       "      <td>Male</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>Motorcycle 125cc and under</td>\n",
       "      <td>Front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200401BS00004</td>\n",
       "      <td>36 - 45</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>U-turn</td>\n",
       "      <td>109</td>\n",
       "      <td>Offside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200401BS00013</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>GOLF V5</td>\n",
       "      <td>Female</td>\n",
       "      <td>Turning right</td>\n",
       "      <td>109</td>\n",
       "      <td>Offside</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Index Age_Band_of_Driver  Age_of_Vehicle  Engine_Capacity_.CC.  \\\n",
       "2  200401BS00003            26 - 35             4.0                 998.0   \n",
       "3  200401BS00003            66 - 75             NaN                   NaN   \n",
       "4  200401BS00004            26 - 35             1.0                 124.0   \n",
       "5  200401BS00004            36 - 45            10.0                1781.0   \n",
       "9  200401BS00013            26 - 35             4.0                2300.0   \n",
       "\n",
       "               make                  model Sex_of_Driver  Vehicle_Manoeuvre  \\\n",
       "2            NISSAN  MICRA CELEBRATION 16V          Male      Turning right   \n",
       "3  LONDON TAXIS INT         TXII GOLD AUTO          Male  Going ahead other   \n",
       "4           PIAGGIO              VESPA ET4          Male  Going ahead other   \n",
       "5        VOLKSWAGEN                    NaN          Male             U-turn   \n",
       "9        VOLKSWAGEN                GOLF V5        Female      Turning right   \n",
       "\n",
       "                 Vehicle_Type X1st_Point_of_Impact  \n",
       "2                         109                Front  \n",
       "3                         109                Front  \n",
       "4  Motorcycle 125cc and under                Front  \n",
       "5                         109              Offside  \n",
       "9                         109              Offside  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateIds = vehicles[vehicles.duplicated(['Accident_Index'], keep=False)] \n",
    "duplicateIds.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the duplicates aren't an issue, as more than one vehicle can be involved in an accident. Now, let's look at the vehicle type, because we are going to create a vehicle dimension table as part of the data model, storing details and linking via an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Car                                      1528628\n",
       "Van / Goods 3.5 tonnes mgw or under       117427\n",
       "109                                        82920\n",
       "Bus or coach (17 or more pass seats)       76757\n",
       "Motorcycle over 500cc                      71472\n",
       "Motorcycle 125cc and under                 61600\n",
       "Goods 7.5 tonnes mgw and over              55426\n",
       "Taxi/Private hire car                      43781\n",
       "Pedal cycle                                38904\n",
       "Motorcycle 50cc and under                  22415\n",
       "Motorcycle over 125cc and up to 500cc      20960\n",
       "Goods over 3.5t. and under 7.5t            18236\n",
       "Other vehicle                              13994\n",
       "106                                         7568\n",
       "Agricultural vehicle                        6018\n",
       "Minibus (8 - 16 passenger seats)            5900\n",
       "Goods vehicle - unknown weight              1876\n",
       "108                                         1334\n",
       "Motorcycle - unknown cc                      741\n",
       "Mobility scooter                             502\n",
       "Data missing or out of range                 401\n",
       "Ridden horse                                 224\n",
       "Electric motorcycle                           78\n",
       "Tram                                          43\n",
       "Name: Vehicle_Type, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the counts on vehicle type\n",
    "vehicles[\"Vehicle_Type\"].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see quite a wide-range of entries in the vehicle type column. There are certainly some cases here where:\n",
    "- A broad category of similar descriptions\n",
    "- not actually a road vehicle\n",
    "\n",
    "With these categories in mind, we will perform some group cleaning on this field. <br> \n",
    "\n",
    "- re-classify 109, 108 & 106 to `Car`\n",
    "- group all the motorcycle types to one `Motorcycle` group \n",
    "- rename `Van / Goods ...` to just `Van` \n",
    "- group all other goods vehicles into one `Goods Vehicle` group \n",
    "- rename `Bus or Coach` to `Bus/Coach` \n",
    "- rename `Minibus (8-16 passenger seats)` to just `Minibus` \n",
    "- change `Data Missing` to `N/A` \n",
    "\n",
    "Place into a new \"clean\" column. We can then drop the old one & rename \n",
    "\n",
    "#### Data Clean - UK Vehicles Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by setting the current column values as default, then override as needed per descriptions above \n",
    "vehicles[\"Vehicle_Type_Clean\"] = vehicles[\"Vehicle_Type\"] \n",
    "\n",
    "# now run through creating new values per list above \n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"106\", \"108\", \"109\"]), 'Vehicle_Type_Clean'] = 'Car' \n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type \\\n",
    "    .isin([\"Motorcycle - unknown cc\", \"Motorcycle 50cc and under\", \"Motorcycle 125cc and under\", \n",
    "            \"Motorcycle over 125cc and up to 500cc\", \"Motorcycle over 500cc\"]), 'Vehicle_Type_Clean'] = 'Motorcycle'\n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"Van / Goods 3.5 tonnes mgw or under\"]), 'Vehicle_Type_Clean'] = 'Van' \n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"Bus or coach (17 or more pass seats)\"]), 'Vehicle_Type_Clean'] = 'Bus/Coach' \n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"Minibus (8 - 16 passenger seats)\"]), 'Vehicle_Type_Clean'] = 'Minibus' \n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"Goods vehicle - unknown weight\", \"Goods over 3.5t. and under 7.5t\", \"Goods 7.5 tonnes mgw and over\"]), 'Vehicle_Type_Clean'] = 'Goods Vehicle' \n",
    "\n",
    "vehicles.loc[vehicles.Vehicle_Type.isin([\"Data missing or out of range\"]), 'Vehicle_Type_Clean'] = 'N/A' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Car                      1620450\n",
       "Motorcycle                177188\n",
       "Van                       117427\n",
       "Bus/Coach                  76757\n",
       "Goods Vehicle              75538\n",
       "Taxi/Private hire car      43781\n",
       "Pedal cycle                38904\n",
       "Other vehicle              13994\n",
       "Agricultural vehicle        6018\n",
       "Minibus                     5900\n",
       "Mobility scooter             502\n",
       "N/A                          401\n",
       "Ridden horse                 224\n",
       "Electric motorcycle           78\n",
       "Tram                          43\n",
       "Name: Vehicle_Type_Clean, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the counts on vehicle type again after changes \n",
    "vehicles[\"Vehicle_Type_Clean\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Age_Band_of_Driver</th>\n",
       "      <th>Age_of_Vehicle</th>\n",
       "      <th>Engine_Capacity_CC</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>Sex_of_Driver</th>\n",
       "      <th>Vehicle_Manoeuvre</th>\n",
       "      <th>X1st_Point_of_Impact</th>\n",
       "      <th>Vehicle_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200401BS00001</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>ROVER</td>\n",
       "      <td>45 CLASSIC 16V</td>\n",
       "      <td>Male</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>Front</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200401BS00002</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BMW</td>\n",
       "      <td>C1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>Front</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200401BS00003</td>\n",
       "      <td>26 - 35</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>NISSAN</td>\n",
       "      <td>MICRA CELEBRATION 16V</td>\n",
       "      <td>Male</td>\n",
       "      <td>Turning right</td>\n",
       "      <td>Front</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Index Age_Band_of_Driver  Age_of_Vehicle  Engine_Capacity_CC  \\\n",
       "0  200401BS00001            26 - 35             3.0              1588.0   \n",
       "1  200401BS00002            26 - 35             NaN                 NaN   \n",
       "2  200401BS00003            26 - 35             4.0               998.0   \n",
       "\n",
       "     make                  model Sex_of_Driver  Vehicle_Manoeuvre  \\\n",
       "0   ROVER         45 CLASSIC 16V          Male  Going ahead other   \n",
       "1     BMW                     C1          Male  Going ahead other   \n",
       "2  NISSAN  MICRA CELEBRATION 16V          Male      Turning right   \n",
       "\n",
       "  X1st_Point_of_Impact Vehicle_Type  \n",
       "0                Front          Car  \n",
       "1                Front          Car  \n",
       "2                Front          Car  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop old vehicle type column, then rename new one \n",
    "vehicles.drop('Vehicle_Type', axis=1, inplace=True) \n",
    "vehicles.rename({'Vehicle_Type_Clean': 'Vehicle_Type'}, axis=1, inplace=True) \n",
    "vehicles.rename({'Engine_Capacity_.CC.': 'Engine_Capacity_CC'}, axis=1, inplace=True) \n",
    "vehicles.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write clean data back to CSV file in clean_data folder \n",
    "# check if the \"clean_data\" directory exists, if not, create it \n",
    "if not os.path.exists(\"clean_data\"):\n",
    "    os.makedirs(\"clean_data\")\n",
    "\n",
    "# write clean data back to clean file \n",
    "vehicles.to_csv(\"clean_data/UK_Vehicles_Information.csv\", sep='~', index=False) \n",
    "\n",
    "del vehicles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the original files, retaining just the clean copies to be used as part of the Data Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem = [\"Accident_Information.csv\", \"Vehicle_Information.csv\", \"DriveTest_Centres.json\", \"LSOA_Data.csv\", \"uk-driving-test-centres.zip\", \"uk-road-safety-accidents-and-vehicles.zip\"]\n",
    "for item in rem:\n",
    "    os.remove(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Conceptual Data Model for the UK Road Accidents Database for the United Kingdom Government, Department of Transport\n",
    "\n",
    "<img src=\"graphics\\Data_Model2.PNG\" width=\"900\" height=\"540\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "#### ETL Process Flow\n",
    "<img src=\"graphics\\Data_Pipeline.PNG\" width=\"1200\" height=\"350\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model. <br>\n",
    "\n",
    "\n",
    "\n",
    "#### Stage 1:\n",
    "Start by using some imported functions from `AWS_S3_Functions` which will set up an S3 bucket on AWS, then load our files to storage there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the config parser to collect details from the AWS.cfg file which will feed into the creation of AWS S3 services \n",
    "config = configparser.ConfigParser() \n",
    "config.read_file(open('AWS.cfg')) \n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "useRegion = config.get('BUCKET','REGION')\n",
    "useBucketName = config.get('BUCKET','NAME') \n",
    "\n",
    "# create S3 AWS client \n",
    "aws_s3_client = boto3.client('s3',\n",
    "                            region_name = useRegion,\n",
    "                            aws_access_key_id = KEY,\n",
    "                            aws_secret_access_key = SECRET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This bucket does not currently exist\n",
      "S3 bucket successfully created\n"
     ]
    }
   ],
   "source": [
    "# check if the bucket already exists \n",
    "bucket_exists = checkS3bucket(s3_client=aws_s3_client, bucketName=useBucketName, clientError=ClientError) \n",
    "# If bucket does not currently exist, attempt to create it \n",
    "if bucket_exists == 'No':\n",
    "    buildIt = buildS3bucket(s3_client=aws_s3_client, bucketName=useBucketName, clientError=ClientError, aws_region=useRegion) \n",
    "    print(buildIt) \n",
    "\n",
    "else: \n",
    "    print(\"Bucket already exists - no creation required\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded UK_Accidents_Information.csv to dmc-021593-capstone\n",
      "Successfully loaded UK_Driving_Test_Centre.json to dmc-021593-capstone\n",
      "Successfully loaded UK_LSOA_Data.csv to dmc-021593-capstone\n",
      "Successfully loaded UK_Vehicles_Information.csv to dmc-021593-capstone\n"
     ]
    }
   ],
   "source": [
    "# With the bucket no successfully created, load data to it (note, this can take up to 10 minutes ... so grab a coffee!)  \n",
    "for file in glob.glob(\"clean_data\\*\"):\n",
    "    fileName = file.split('\\\\')[1] \n",
    "    loaded_file = loadToS3(s3_client=aws_s3_client, bucketName=useBucketName, clientError=ClientError, bucketKey='project_data/', loadFile=file, fileName=fileName) \n",
    "    print(loaded_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2:\n",
    "Now that the data is stored in AWS S3, we will set up a redshift cluster & db, which will contain the staging tables and end user analyst tables. <br>\n",
    "We will need to create some AWS clients to work with. This will involve creating a specified IAM role to create a Redshift cluster & have read-only access to the S3 bucket created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_redshift_client = boto3.client('redshift',\n",
    "                            region_name = useRegion,\n",
    "                            aws_access_key_id = KEY,\n",
    "                            aws_secret_access_key = SECRET) \n",
    "\n",
    "aws_iam_client = boto3.client('iam',\n",
    "                            region_name = useRegion,\n",
    "                            aws_access_key_id = KEY,\n",
    "                            aws_secret_access_key = SECRET)\n",
    "\n",
    "aws_ec2_resource = boto3.resource('ec2',\n",
    "                            region_name=useRegion,\n",
    "                            aws_access_key_id = KEY,\n",
    "                            aws_secret_access_key = SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new IAM Role ...\n",
      "IAM role etldev successfully created\n",
      "Attaching Policy to IAM role ...\n",
      "Policy attached to IAM role\n"
     ]
    }
   ],
   "source": [
    "# Create the IAM role choosing a name & policy \n",
    "roleName = config.get('AWS','IAM_ROLE')\n",
    "policy = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" \n",
    "try:\n",
    "    print(\"Creating a new IAM Role ...\") \n",
    "    dwhRole = aws_iam_client.create_role(\n",
    "        Path='/',\n",
    "        RoleName=roleName,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "    print(\"IAM role \" + str(roleName) + \" successfully created\")\n",
    "except Exception as e:\n",
    "    print(\"IAM role creation failed\") \n",
    "    print(e) \n",
    "\n",
    "# Attach the policy to the IAM role \n",
    "try:\n",
    "    print(\"Attaching Policy to IAM role ...\")\n",
    "    aws_iam_client.attach_role_policy(RoleName=roleName,\n",
    "                        PolicyArn=policy)['ResponseMetadata']['HTTPStatusCode'] \n",
    "    print(\"Policy attached to IAM role\") \n",
    "except Exception as e:\n",
    "    print(\"IAM role policy attach failed\") \n",
    "    print(e) \n",
    "\n",
    "# Collect the ARN details of the IAM role \n",
    "roleArn = aws_iam_client.get_role(RoleName=roleName)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the IAM role now created, launch a AWS redshift cluster. <br> \n",
    "The details for the clusters configuration can be collected from the AWS.cfg file <br>\n",
    "#### Create Redshift Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Redhsift cluster dmc0293UKaccidents ...\n",
      "Cluster dmc0293UKaccidents has been created\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Creating Redhsift cluster \" + str(config.get('DWH','DWH_CLUSTER_IDENTIFIER')) + \" ...\") \n",
    "    response = aws_redshift_client.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=config.get('DWH','DWH_CLUSTER_TYPE'),\n",
    "        NodeType=config.get('DWH','DWH_NODE_TYPE'),\n",
    "        NumberOfNodes=int(config.get('DWH','DWH_NUM_NODES')),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=config.get('DWH','DWH_DB'),\n",
    "        ClusterIdentifier=config.get('DWH','DWH_CLUSTER_IDENTIFIER'),\n",
    "        MasterUsername=config.get('DWH','DWH_DB_USER'),\n",
    "        MasterUserPassword=config.get('DWH','DWH_DB_PASSWORD'),\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "    print(\"Cluster \" + str(config.get('DWH','DWH_CLUSTER_IDENTIFIER')) + \" has been created\") \n",
    "except Exception as e:\n",
    "    print(\"Cluster creation has failed\") \n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dmc0293ukaccidents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dbadmin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>uk_accidents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-22e0c85a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1           NodeType   \n",
       "2      ClusterStatus   \n",
       "3     MasterUsername   \n",
       "4             DBName   \n",
       "5           Endpoint   \n",
       "6              VpcId   \n",
       "7      NumberOfNodes   \n",
       "\n",
       "                                                                                           Value  \n",
       "0                                                                             dmc0293ukaccidents  \n",
       "1                                                                                      dc2.large  \n",
       "2                                                                                      available  \n",
       "3                                                                                        dbadmin  \n",
       "4                                                                                   uk_accidents  \n",
       "5  {'Address': 'dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6                                                                                   vpc-22e0c85a  \n",
       "7                                                                                              4  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the following function to check for an active cluster - ClusterStatus must change from `creating` to `available` in order to be ready for use \n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = aws_redshift_client.describe_clusters(ClusterIdentifier=config.get('DWH','DWH_CLUSTER_IDENTIFIER'))['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects the Endpoint & ARN \n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens an incoming TCP port to access the cluster \n",
    "# try:\n",
    "#     useVpc = myClusterProps['VpcId'] \n",
    "#     vpc = aws_ec2_resource.Vpc(id=useVpc)\n",
    "#     defaultSg = list(vpc.security_groups.all())[0]\n",
    "#     print(defaultSg)\n",
    "#     try:\n",
    "#         defaultSg.authorize_ingress(\n",
    "#             GroupName=defaultSg.group_name,\n",
    "#             CidrIp='0.0.0.0/0',\n",
    "#             IpProtocol='TCP',\n",
    "#             FromPort=int(config.get('DWH','DWH_PORT')),\n",
    "#             ToPort=int(config.get('DWH','DWH_PORT'))\n",
    "#         )\n",
    "#     except:\n",
    "#         pass \n",
    "#     print(\"TCP port open\") \n",
    "# except Exception as e:\n",
    "#     print(\"Opening TCP Port failed\") \n",
    "#     print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the Endpoint (aka Host) and the ARN out to the AWS.cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = 'AWS.cfg' \n",
    "config = configparser.ConfigParser()\n",
    "config.read(cfg_file)\n",
    "\n",
    "def set_config(sec, attr, value):\n",
    "    config.set(sec, attr, value)\n",
    "    with open(cfg_file, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "\n",
    "\n",
    "set_config('CLUSTER', 'HOST', DWH_ENDPOINT)\n",
    "set_config('CLUSTER', 'ROLE_ARN', DWH_ROLE_ARN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 3:\n",
    "So, with data now stored in an S3 bucket, and a redshift cluster & database created, we can run the main stages of the ETL process. <br>\n",
    "This will be the create tables stage, which will build a series of empty tables, before then actually loading those tables with data. <br>\n",
    "This pattern will be done in two parts:\n",
    "- First, staging tables. Dropped if pre-existing, then created, then copied into \n",
    "- Second analytics tables, Dropped if pre-existing, then created, then inserted into from the staging tables \n",
    "\n",
    "Finally, we will \"clean up\" the database by removing the unrequired staging tables once the analytics tables are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run staging_tables.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the staging tables have been built, we can create and load data for the analytical tables via the next script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run analytics_tables.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run a clean up script, where all staging tables are removed after the analytics tables have been created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run drop_staging_tables.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "1 - Run a check to ensure that no duplicates exist for the combination of the `Accident_Index` & `Vehicle_Number` column from the Accidents FACT table <br>\n",
    "2 - Run a check to ensure that the Vehicles DIMENSION table contains *Car Van, Motorcycle & Bus/Coach* within the `Vehicle_Type` column, as they are they main categories that should exist <br>\n",
    "3 - Run a check to ensure no NULL values in `Latitude` & `Longitude` within the Test Centres DIMENSION table <br>\n",
    "4 - Run a check to ensure that there are no duplicates in the `LSOA code` & `Constituency` combinations within the Locations DIMENSION table <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the magic sql module for the notebook \n",
    "%load_ext sql \n",
    "#%reload_ext sql\n",
    "\n",
    "# set connection details to redshift db \n",
    "usr = config.get('DWH','dwh_db_user')\n",
    "pw = config.get('DWH','dwh_db_password') \n",
    "ep = config.get('CLUSTER','host') \n",
    "port = config.get('DWH','dwh_port') \n",
    "db = config.get('DWH','dwh_db') \n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(usr, pw, ep, port, db)\n",
    "#print(conn_string)\n",
    "%sql $conn_string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Quality check 1 - `duplicate_records` should = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dbadmin:***@dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com:5439/uk_accidents\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>duplicate_records</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0,)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(a.*) as duplicate_records\n",
    "FROM (SELECT Accident_Index, Vehicle_Number, count(*) as count_ FROM accidents GROUP BY Accident_Index, Vehicle_Number) as a\n",
    "WHERE a.count_ > 1 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Quality check 2 - Vehicles DIMENSION table DOES contain *Car Van, Motorcycle & Bus/Coach* within the `Vehicle_Type` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dbadmin:***@dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com:5439/uk_accidents\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>vehicle_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bus/Coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Motorcycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Van</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Bus/Coach',), ('Motorcycle',), ('Car',), ('Van',)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT DISTINCT vehicle_type FROM Vehicles \n",
    "WHERE vehicle_type IN ('Car', 'Van', 'Motorcycle', 'Bus/Coach') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Quality check 3 - ensure no NULL values in `Latitude` & `Longitude` within the Test Centres DIMENSION table <br> \n",
    "count result should equal 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dbadmin:***@dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com:5439/uk_accidents\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>null_records</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0,)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT COUNT(*) as NULL_Records\n",
    "FROM driving_test_centres \n",
    "WHERE latitude IS NULL or longitude IS NULL ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Quality check 4 - check to ensure that there are no duplicates in the `LSOA` code within the Locations DIMENSION table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dbadmin:***@dmc0293ukaccidents.cms6ffzqbc3y.us-west-2.redshift.amazonaws.com:5439/uk_accidents\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>duplicate_records</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0,)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(a.*) as duplicate_records\n",
    "FROM (SELECT lsoa, constituency, count(*) as count_ FROM locations GROUP BY lsoa, constituency) as a\n",
    "WHERE a.count_ > 1 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# close the connection\n",
    "connections = %sql -l\n",
    "[c.session.close() for c in connections.values()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "#### Accidents\n",
    "| *Column* | *Type* | *Description* |\n",
    "| :- | :- | :- |\n",
    "| Accident_Index | VARCHAR | An alphanumeric key to indicate a Traffic Accident Record. Combined with the `Vehicle_Number`, it creates a **<font color='red'>Primary Key</font>** |\n",
    "| Vehicle_Number | INT | A numeric key to signal which vehicle involved in the accident this represents. Combined with the `Accident_Index`, it creates a **<font color='red'>Primary Key</font>** |\n",
    "| Vehicle_Key | INT | A unique key to link to the `Vehicles` Table. Details on the vehicle involved in the accident |\n",
    "| DateTime_Key | VARCHAR | A unique key to link to the `DateTime` Table. Date & time of the accident |\n",
    "| Latitude | VARCHAR | The latitude co-ordinates of where the accident occurred |\n",
    "| Longitude | VARCHAR | The longitude co-ordinates of where the accident occurred |\n",
    "| LSOA | VARCHAR | The Lower Layer Super Output Area (a geographic hierarchy in the UK) of where the accident occurred |\n",
    "| First_Road_Class | VARCHAR | The classification of the road the accident occurred on. This, combined with the `First_Road_Number` provides the road identification (except for unlclassified roads) |\n",
    "| First_Road_Number | VARCHAR | The number of the road the accident occurred on. This, combined with `First_Road_Class` provides the road identification (except for unlclassified roads) |\n",
    "| Road_Surface_Conditions | VARCHAR | Road surface conditions at accident site |\n",
    "| Weather_Conditions | VARCHAR | Weather conditions at accident site |\n",
    "| Police_Attended | VARCHAR | Did police officer attend scene at accident site |\n",
    "| Number_of_Casualties | VARCHAR | Number of casualties from accident |\n",
    "| Number_of_Vehicles | VARCHAR | Number of vehicles involved in accident |\n",
    "| Accident_Severity | VARCHAR | The severity of the accident |\n",
    "| Age_Band_of_Driver | VARCHAR | Age band of driver of this vehicle, in this accident |\n",
    "| Sex_of_Driver | VARCHAR | Sex of driver of this vehicle, in this accident |\n",
    "| Age_of_Vehicle | VARCHAR | Age of vehicle involved in accident |\n",
    "| First_Point_of_Impact | VARCHAR | First point of impact of vehicle involved in accident |\n",
    "\n",
    "#### Vehicles\n",
    "| *Column* | *Type* | *Description* |\n",
    "| :- | :- | :- |\n",
    "| Vehicle_Key | INT | A unique key to link the Vehicles DIM table to the `Accidents` table **<font color='red'>Primary Key</font>** |\n",
    "| Vehicle_Type | VARCHAR | The type of vehicle involved in the accident. This can be values such as a Car, Van, Motorcycle, Bus, or in some cases, non-vehicles where involved in accidents, i.e a Horse |\n",
    "| Make | VARCHAR | The vehicle manufacturer, for example Nissan or Audi |\n",
    "| Model | VARCHAR | The vehicle model, for example Juke (Nissan) or Q5 (Audi) |\n",
    "| Engine_Capacity | VARCHAR | The CC of the engine of the vehicle. Not Applicable for non-vehicles |\n",
    "\n",
    "#### Driving_Test_Centres \n",
    "| *Column* | *Type* | *Description* |\n",
    "| :- | :- | :- |\n",
    "| Latitude | VARCHAR | The latitude co-ordinates of where the test centre is based **<font color='red'>Primary Key</font>** |\n",
    "| Longitude | VARCHAR | The longitude co-ordinates of where the test centre is based **<font color='red'>Primary Key</font>** |\n",
    "| Name | VARCHAR | Name of the Test Centre **<font color='red'>Primary Key</font>** |\n",
    "| City | VARCHAR | City the Test Centre is in |\n",
    "| Postcode | VARCHAR | Postcode of the Test Centre Address |\n",
    "\n",
    "#### Locations \n",
    "| *Column* | *Type* | *Description* |\n",
    "| :- | :- | :- |\n",
    "| LSOA | VARCHAR | Lower Layer Super Output Area (a geographic hierarchy in the UK) providing a mapping key for granular locations for reporting purposes via UK ONS **<font color='red'>Primary Key</font>** |\n",
    "| Constituency | VARCHAR | The parliamentary name for a specific area/boundary which makes up one of 650 within the UK Government **<font color='red'>Primary Key</font>** |\n",
    "| Country | VARCHAR | England, Scotland, Wales or Northern Ireland - the countries making up the United Kingdom |\n",
    "| Police_Force | VARCHAR | The name of the relevant police force operating in the LSOA area |\n",
    "| Avg_Income | DECIMAL(20.2) | The average income of an individual within the LSOA area |\n",
    "\n",
    "#### DateTime \n",
    "| *Column* | *Type* | *Description* |\n",
    "| :- | :- | :- |\n",
    "| DateTime_Key | VARCHAR | A unique key to link the DateTime DIM table to the `Accidents` table **<font color='red'>Primary Key</font>**| \n",
    "| Date | DATE | The date of the accident |\n",
    "| Time | VARCHAR | The time of the accident |\n",
    "| Year | VARCHAR | The year of the accident |\n",
    "| Month | VARCHAR | The month of the accident |\n",
    "| Day | VARCHAR | The day of the accident |\n",
    "| Weekday | VARCHAR | The weekday of the accident |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "### <font color='Red'>**Clean Up ETL resources**</font> \n",
    "\n",
    "- Delete Cluster\n",
    "- Delete IAM role \n",
    "- Delete S3 bucket\n",
    "- Delete Clean Data stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Deleted\n"
     ]
    }
   ],
   "source": [
    "# delete cluster \n",
    "try:\n",
    "    aws_redshift_client.delete_cluster(ClusterIdentifier=config.get('DWH','DWH_CLUSTER_IDENTIFIER'),  SkipFinalClusterSnapshot=True) \n",
    "    print(\"Cluster Deleted\") \n",
    "except Exception as e:\n",
    "    print(\"Cluster Deletion has failed\")\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM Policy removed\n",
      "------------------\n",
      "IAM Role deleted\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# remove IAM role created \n",
    "try:\n",
    "    aws_iam_client.detach_role_policy(RoleName=config.get('AWS','IAM_ROLE'), PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    print(\"IAM Policy removed\") \n",
    "    print(\"------------------\")\n",
    "    aws_iam_client.delete_role(RoleName=config.get('AWS','IAM_ROLE')) \n",
    "    print(\"IAM Role deleted\") \n",
    "    print(\"------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to delete IAM role\") \n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting project_data/UK_Accidents_Information.csv\n",
      "Deleting project_data/UK_Driving_Test_Centre.json\n",
      "Deleting project_data/UK_LSOA_Data.csv\n",
      "Deleting project_data/UK_Vehicles_Information.csv\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# empty S3 bucket \n",
    "try:\n",
    "    BUCKET = config.get('BUCKET','NAME')\n",
    "    PREFIX = 'project_data/'\n",
    "\n",
    "    response = aws_s3_client.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)\n",
    "\n",
    "    for object in response['Contents']:\n",
    "        print('Deleting', object['Key'])\n",
    "        aws_s3_client.delete_object(Bucket=BUCKET, Key=object['Key']) \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"S3 Bucket failed to empty\")\n",
    "    print(e)\n",
    "\n",
    "print(\"--------------------------------------------------------------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket has been deleted\n"
     ]
    }
   ],
   "source": [
    "aws_s3_resource = boto3.resource('s3',\n",
    "                            region_name = useRegion,\n",
    "                            aws_access_key_id = KEY,\n",
    "                            aws_secret_access_key = SECRET) \n",
    "\n",
    "try:\n",
    "    aws_s3_resource.Bucket(BUCKET).delete() \n",
    "    print(\"S3 bucket has been deleted\") \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"S3 bucket failed to delete\") \n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove clean data from local storage \n",
    "clean_list = [r\"clean_data\\UK_Accidents_Information.csv\", r\"clean_data\\UK_Vehicles_Information.csv\", r\"clean_data\\UK_LSOA_Data.csv\", r\"clean_data\\UK_Driving_Test_Centre.json\"]\n",
    "for clean_file in clean_list:\n",
    "    os.remove(clean_file) \n",
    "# end   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Tools & Technologies</font>\n",
    "\n",
    "During the initial stages of data sourcing, exploration and cleaning, the Pandas library was chosen as the main tool. This package has a wide range of abilities. It is able to source & read in data from a variety of file formats, such as TXT, CSV & JSON used in this project, but can also access web URLs too for files stored on the web rather than a local network/folder. <br>\n",
    "Pandas also proves easy to use with exploring data in its dataframe format. Column values can be explored for values, duplicates and counts. New columns or cleaned columns can be easily created and added to an existing dataframe, or summarised results can be written to a new dataframe. <br>\n",
    "These cleaned/summarised dataframes can easily be written back out to different file formats, such as CSV or JSON. <br>\n",
    "\n",
    "With regards to the ETL process, I chose to utilsie AWS S3 as the storage for the `source data`. AWS S3 offers large, cheap and easy to access storage for all files types, on the cloud. This proves beneficial as it allows easy access to other engineers with the correct access keys, provides ample storage space for a variety of file types, and can be easily connected to work with other AWS products or tools. <br> \n",
    "In order to automate as much as possible, Python to AWS SDKs were used, to create an S3 bucket and load the gathered data sources to that bucket for future use. <br>\n",
    "With the bucket created, and cleaned source data stored, the next decision was to choose a way to ETL the data and place into analytical tables for future use. <br> \n",
    "\n",
    "The next stage of the ETL process uses the Python AWS SDK again, this time, so create a specific IAM role with S3 Read access, in order to pull the stored data. A redshift cluster is also created. <br> \n",
    "Within this cluster, we create a series of staging tables where we copy data from S3 to redshift (bulk loaded for quick copy). These staging tables prove a useful intermediate step, allowing the pipeline to access different data from each of the staging tables to build the analytical tables that analysts can use for MI reporting and querying. <br> \n",
    "\n",
    "Redshift has the capability to be scaled in different sizes depending on need and use case, so as large data builds when collecting a history over time, or more computational power is needed, this can be achieved. <br> \n",
    "\n",
    "The data model chosen, a star schema, seemed appropriate as we had key event data, the traffic accidents themselves, alongside multiple reference points. By providing dimension tables covering vehicle details (make, model, engine size), location details (areas, constituencies for Parliament Reporting), Income and Local Driving Test Centres to accident spots, a variety of different insights can be achieved. Some examples of those may be:\n",
    "\n",
    "- Which constituencies see the most accidents? Is there also a link to low/high income areas?\n",
    "- Are there driving test centres near-by? does this suggest those taking a test could be involved? are some test centres maybe passing drivers who are not quite ready? There could be some initial exploratory work to look into this\n",
    "- what vehicles are most frequently found in accidents? which motorways or A roads are most affected?\n",
    "\n",
    "\n",
    "### <font color='green'>Updating Data</font> \n",
    "\n",
    "The sourced data for this project has different intervals. <br>\n",
    "The Postcode data containing LSOA codes, average incomes and police force details updates each quarter, in a release by the UK ONS. <br>\n",
    "Accidents information updates daily, so in a real production pipeline, we would almost certainly need to update the pipeline daily to keep up with new records, unless storing daily records, to then load in monthly or quarterly loads. <br> \n",
    "Driving Test Centres data is more static, with the UK government releasing a new updated list each quarter. <br>\n",
    "\n",
    "With all these factors in mind, it seems sensible to still update daily to capture the key records, but understand that much of the reference data will only change quarterly, but most of the FACT data will change & add daily.\n",
    "\n",
    "\n",
    "### <font color='green'>Approaching Different Scenarios</font>\n",
    "#### Data Size increased by 100x\n",
    "The use of AWS redshift allows this pipeline to scale quickly to handle larger data storage requirements within the database for analyst use. This also allows compute to be scaled to handle the users running larger queries. <br>\n",
    "\n",
    "#### The data is required for a dashboard at 7am each day \n",
    "This could be achieved using a tool like Apache Airflow, which can schedule a daily run in the early hours of each morning, to collect the latest data and push that data through the ETL process and into the UK Accidents Database. These tables can then feed the downstream dashboard as required.\n",
    "\n",
    "#### The database needed to be accessed by 100+ people\n",
    "AWS redshift again proves useful in this scenario, as we can scale up the cluster(s) for more concurrent use. By running a larger cluster size on the cloud, we can quickly achieve the desired cluster size needed to perform multiple queries by hundreds of analysts accessing data. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcdb6e67d461e2efab67d4fa6e0571bdfed59e5698c03213fe392628926ef1fe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
